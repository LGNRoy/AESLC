{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "\n",
    "import subprocess\n",
    "from collections import Counter\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import torch\n",
    "from multiprocess import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from others.logging import logger\n",
    "from others.tokenization import BertTokenizer\n",
    "\n",
    "from prepro.utils import _get_word_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token = '[SEP]'\n",
    "cls_token = '[CLS]'\n",
    "pad_token = '[PAD]'\n",
    "tgt_bos = '[unused0]'\n",
    "tgt_eos = '[unused1]'\n",
    "tgt_sent_split = '[unused2]'\n",
    "sep_vid = tokenizer.vocab[sep_token]\n",
    "cls_vid = tokenizer.vocab[cls_token]\n",
    "pad_vid = tokenizer.vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_src_ntokens_per_sent = 1\n",
    "max_src_ntokens_per_sent = 600\n",
    "min_src_nsents = 1\n",
    "max_src_nsents = 200\n",
    "\n",
    "min_tgt_ntokens = 1\n",
    "max_tgt_ntokens = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cpus = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = True\n",
    "use_bert_basic_tokenizer = True\n",
    "\n",
    "toke_path = \"random_tokened/token/\"\n",
    "save_bath = \"random_tokened/bert/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertData():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "        self.sep_token = '[SEP]'\n",
    "        self.cls_token = '[CLS]'\n",
    "        self.pad_token = '[PAD]'\n",
    "        self.tgt_bos = '[unused0]'\n",
    "        self.tgt_eos = '[unused1]'\n",
    "        self.tgt_sent_split = '[unused2]'\n",
    "        self.sep_vid = self.tokenizer.vocab[self.sep_token]\n",
    "        self.cls_vid = self.tokenizer.vocab[self.cls_token]\n",
    "        self.pad_vid = self.tokenizer.vocab[self.pad_token]\n",
    "\n",
    "    def preprocess(self, src, tgt, sent_labels, use_bert_basic_tokenizer=False, is_test=False):\n",
    "\n",
    "        if ((not is_test) and len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > min_src_ntokens_per_sent)]\n",
    "\n",
    "        _sent_labels = [0] * len(src)\n",
    "        for l in sent_labels:\n",
    "            _sent_labels[l] = 1\n",
    "\n",
    "        src = [src[i][:max_src_ntokens_per_sent] for i in idxs]\n",
    "        sent_labels = [_sent_labels[i] for i in idxs]\n",
    "        src = src[:max_src_nsents]\n",
    "        sent_labels = sent_labels[:max_src_nsents]\n",
    "\n",
    "        if ((not is_test) and len(src) < min_src_nsents):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        text = ' {} {} '.format(self.sep_token, self.cls_token).join(src_txt)\n",
    "\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "        src_subtokens = [self.cls_token] + src_subtokens + [self.sep_token]\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        sent_labels = sent_labels[:len(cls_ids)]\n",
    "\n",
    "        tgt_subtokens_str = '[unused0] ' + ' [unused2] '.join(\n",
    "            [' '.join(self.tokenizer.tokenize(' '.join(tt), use_bert_basic_tokenizer=use_bert_basic_tokenizer)) for tt in tgt]) + ' [unused1]'\n",
    "        tgt_subtoken = tgt_subtokens_str.split()[:max_tgt_ntokens]\n",
    "        if ((not is_test) and len(tgt_subtoken) < min_tgt_ntokens):\n",
    "            return None\n",
    "\n",
    "        tgt_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(tgt_subtoken)\n",
    "\n",
    "        tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "\n",
    "        return src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_bert(args):\n",
    "    datasets = ['train', 'test', 'dev']\n",
    "    for corpus_type in datasets:\n",
    "        a_lst = []\n",
    "        for json_f in glob.glob(pjoin(args.raw_path, '*' + corpus_type + '.*.json')):\n",
    "            real_name = json_f.split('/')[-1]\n",
    "            a_lst.append((corpus_type, json_f, args, pjoin(args.save_path, real_name.replace('json', 'bert.pt'))))\n",
    "        pool = Pool(args.n_cpus)\n",
    "        for d in pool.imap(_format_to_bert, a_lst):\n",
    "            pass\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_to_bert(params):\n",
    "    corpus_type, json_file, args, save_file = params\n",
    "    is_test = corpus_type == 'test'\n",
    "    if (os.path.exists(save_file)):\n",
    "        logger.info('Ignore %s' % save_file)\n",
    "        return\n",
    "\n",
    "    bert = BertData(args)\n",
    "\n",
    "    logger.info('Processing %s' % json_file)\n",
    "    jobs = json.load(open(json_file))\n",
    "    datasets = []\n",
    "    for d in jobs:\n",
    "        source, tgt = d['src'], d['tgt']\n",
    "\n",
    "        sent_labels = greedy_selection(source[:args.max_src_nsents], tgt, 3)\n",
    "        if (args.lower):\n",
    "            source = [' '.join(s).lower().split() for s in source]\n",
    "            tgt = [' '.join(s).lower().split() for s in tgt]\n",
    "        b_data = bert.preprocess(source, tgt, sent_labels, use_bert_basic_tokenizer=args.use_bert_basic_tokenizer,\n",
    "                                 is_test=is_test)\n",
    "        # b_data = bert.preprocess(source, tgt, sent_labels, use_bert_basic_tokenizer=args.use_bert_basic_tokenizer)\n",
    "\n",
    "        if (b_data is None):\n",
    "            continue\n",
    "        src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "        b_data_dict = {\"src\": src_subtoken_idxs, \"tgt\": tgt_subtoken_idxs,\n",
    "                       \"src_sent_labels\": sent_labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                       'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "        datasets.append(b_data_dict)\n",
    "    logger.info('Processed instances %d' % len(datasets))\n",
    "    logger.info('Saving to %s' % save_file)\n",
    "    torch.save(datasets, save_file)\n",
    "    datasets = []\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_rouge(evaluated_ngrams, reference_ngrams):\n",
    "    reference_count = len(reference_ngrams)\n",
    "    evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "    overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "    if evaluated_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = overlapping_count / evaluated_count\n",
    "\n",
    "    if reference_count == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = overlapping_count / reference_count\n",
    "\n",
    "    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "    return {\"f\": f1_score, \"p\": precision, \"r\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedyselect(doc_sent_list, sub_sent, summary_size):\n",
    "    def _rouge_clean(s):\n",
    "        return re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "    max_rouge = 0.0\n",
    "    subject = _rouge_clean(' '.join(sub_sent)).lower().split()\n",
    "    sents = [_rouge_clean(' '.join(s)).lower().split() for s in doc_sent_list]\n",
    "    evaluated_1grams = [_get_word_ngrams(1, [sent]) for sent in sents]\n",
    "    reference_1grams = _get_word_ngrams(1, [subject])\n",
    "    evaluated_2grams = [_get_word_ngrams(2, [sent]) for sent in sents]\n",
    "    reference_2grams = _get_word_ngrams(2, [subject])\n",
    "\n",
    "    selected = []\n",
    "    for s in range(summary_size):\n",
    "        cur_max_rouge = max_rouge\n",
    "        cur_id = -1\n",
    "        for i in range(len(sents)):\n",
    "            if (i in selected):\n",
    "                continue\n",
    "            c = selected + [i]\n",
    "            candidates_1 = [evaluated_1grams[idx] for idx in c]\n",
    "            candidates_1 = set.union(*map(set, candidates_1))\n",
    "            candidates_2 = [evaluated_2grams[idx] for idx in c]\n",
    "            candidates_2 = set.union(*map(set, candidates_2))\n",
    "\n",
    "            rouge_1 = cal_rouge(candidates_1, reference_1grams)['f']\n",
    "            rouge_2 = cal_rouge(candidates_2, reference_2grams)['f']\n",
    "            rouge_score = rouge_1 + rouge_2\n",
    "\n",
    "            if rouge_score > cur_max_rouge:\n",
    "                cur_max_rouge = rouge_score\n",
    "                cur_id = i\n",
    "        if (cur_id == -1):\n",
    "            return selected\n",
    "        selected.append(cur_id)\n",
    "        max_rouge = cur_max_rouge\n",
    "    return sorted(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus_type in [\"dev\",\"test\",\"train\"]:\n",
    "    toke_file = \"{}AESLC_{}_token_{}.txt\".format(toke_path, corpus_type,\"*\")\n",
    "    save_file = \"{}AESLC.bert.{}.pt\".format(save_bath, corpus_type)\n",
    "\n",
    "    reader_doc = open(\"{}AESLC_{}_token_{}.txt\".format(toke_path, corpus_type, \"doc\"))\n",
    "    reader_sub = open(\"{}AESLC_{}_token_{}.txt\".format(toke_path, corpus_type, \"sub\"))\n",
    "\n",
    "    is_test = corpus_type == 'test'\n",
    "\n",
    "    bert = BertData()\n",
    "    datasets = []\n",
    "\n",
    "    while True:\n",
    "        doc_token = reader_doc.readline().replace(\"\\n\",\"\")\n",
    "        sub_token = reader_sub.readline().replace(\"\\n\",\"\")\n",
    "        if not doc_token:\n",
    "            reader_doc.close()\n",
    "            reader_sub.close()\n",
    "            break\n",
    "\n",
    "        doc = [tokens.split(\"<tokesep>\") for tokens in doc_token.split(\"<sentsep>\")]\n",
    "        sub = sub_token.replace(\"<sentsep>\",\"<tokesep>\").split(\"<tokesep>\")\n",
    "\n",
    "        sent_labels = greedyselect(doc[:max_src_nsents], sub, 1)\n",
    "        if (lower):\n",
    "            doc = [' '.join(s).lower().split() for s in doc]\n",
    "            sub = [' '.join(sub).lower().split()]\n",
    "\n",
    "        b_data = bert.preprocess(doc, sub, sent_labels, use_bert_basic_tokenizer=use_bert_basic_tokenizer, is_test=is_test)\n",
    "\n",
    "        if (b_data is None):\n",
    "            continue\n",
    "        src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt = b_data\n",
    "        b_data_dict = {\"src\": src_subtoken_idxs, \"tgt\": tgt_subtoken_idxs,\n",
    "                       \"src_sent_labels\": sent_labels, \"segs\": segments_ids, 'clss': cls_ids,\n",
    "                       'src_txt': src_txt, \"tgt_txt\": tgt_txt}\n",
    "        datasets.append(b_data_dict)\n",
    "\n",
    "    logger.info('Processed instances %d' % len(datasets))\n",
    "    logger.info('Saving to %s' % save_file)\n",
    "    torch.save(datasets, save_file)\n",
    "    datasets = []\n",
    "    gc.collect()\n",
    "\n",
    "    reader_doc.close()\n",
    "    reader_sub.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 231508/231508 [00:00<00:00, 332968.00B/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, cache_dir='../temp')\n",
    "symbols = {'BOS': tokenizer.vocab['[unused0]'], 'EOS': tokenizer.vocab['[unused1]'],\n",
    "           'PAD': tokenizer.vocab['[PAD]'], 'EOQ': tokenizer.vocab['[unused2]']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(line):\n",
    "    print([tokenizer.ids_to_tokens[int(n)] for n in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'randim_tokened/bert/AESLC.bert.test.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ad5b61a603db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"randim_tokened/bert/AESLC.bert.test.pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tgt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'randim_tokened/bert/AESLC.bert.test.pt'"
     ]
    }
   ],
   "source": [
    "test = torch.load(\"random_tokened/bert/AESLC.bert.test.pt\")\n",
    "decode(test[0]['tgt'])\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[unused0]', 'h', 'u', 'n', 't', 'l', 'e', 'y', '[unused2]', '/', '[unused2]', 'q', 'u', 'e', 's', 't', 'i', 'o', 'n', '[unused1]']\n",
      "1906\n"
     ]
    }
   ],
   "source": [
    "test = torch.load(\"../bertAeslc/bert_data/bert.test.pt\")\n",
    "decode(test[0]['tgt'])\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
